{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYmkZZ8n0SVK"
      },
      "source": [
        "#Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_9_qHhI0X3a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.efficientnet import EfficientNetB5\n",
        "from keras.applications import imagenet_utils\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCHk2IZP9PnU"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "root_dir = '/content/drive/My Drive/data'\n",
        "\n",
        "dataset = datasets.ImageFolder(root=root_dir, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuAWl5Cn0y7F"
      },
      "source": [
        "#Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "root_dir = '/content/drive/My Drive/data'\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.images[idx])\n",
        "        image = cv2.imread(img_name)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        label = int(self.images[idx].split('_')[0])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define data augmentation pipeline\n",
        "augmentation = iaa.Sequential([\n",
        "    iaa.SomeOf((0, 3), [\n",
        "        # Your augmentation techniques here\n",
        "    ])\n",
        "], random_order=True)\n",
        "\n",
        "# Define the transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    augmentation,\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create an instance of your custom dataset with the defined augmentation\n",
        "custom_dataset = CustomDataset(root_dir=root_dir, transform=transform)\n",
        "\n",
        "# Define the path to save the augmented dataset\n",
        "save_path = '/content/drive/My Drive/custom_data'\n",
        "\n",
        "# Create the custom_data directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save the augmented dataset to a new file in your Google Drive\n",
        "torch.save(custom_dataset, os.path.join(save_path, 'augmented_dataset.pth'))\n"
      ],
      "metadata": {
        "id": "7kB1xrU6YgI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "root_dir = '/content/drive/My Drive/data'\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.images[idx])\n",
        "        image = cv2.imread(img_name)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        label = int(self.images[idx].split('_')[0])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "augmentation = iaa.Sequential([\n",
        "    iaa.SomeOf((0, 3), [])\n",
        "], random_order=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    augmentation,\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "custom_dataset = CustomDataset(root_dir=root_dir, transform=transform)\n",
        "\n",
        "save_path = '/content/drive/My Drive/custom_data'\n",
        "\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "torch.save(custom_dataset, os.path.join(save_path, 'augmented_dataset.pth'))\n"
      ],
      "metadata": {
        "id": "kMlBS8zAga2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vw7x4620u1J"
      },
      "source": [
        "#Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# root_dir2 = '/content/drive/My Drive/custom_data'\n",
        "# dataset = datasets.ImageFolder(root=root_dir2, transform=transform)"
      ],
      "metadata": {
        "id": "xMTxBhHLZYXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9W0nScpzfkS"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into train, val, and test sets\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.7 * dataset_size)\n",
        "val_size = int(0.15 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stpt35a00-Rj"
      },
      "source": [
        "#Building the Model - (ResNet50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv6U3PB5093c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "class ResNet50_Transformer(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(ResNet50_Transformer, self).__init__()\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        self.embedding = nn.Linear(2048, 512)\n",
        "        self.transformer_layers = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "        self.transformer = nn.TransformerEncoder(self.transformer_layers, num_layers=4)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.resnet(x)\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA5Ybzib2K7E"
      },
      "source": [
        "#Building Vit (Vision Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh9WY04S6OUl",
        "outputId": "a31f9b50-9fad-405b-ce27-c3233d5dce1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionTransformer(\n",
            "  (embedding): Linear(in_features=196608, out_features=512, bias=True)\n",
            "  (transformer_layers): TransformerEncoderLayer(\n",
            "    (self_attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "    )\n",
            "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout1): Dropout(p=0.1, inplace=False)\n",
            "    (dropout2): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchvision.models import resnet50\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_classes=5):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_channels * 256 * 256, 512)\n",
        "        self.transformer_layers = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "        self.transformer = nn.TransformerEncoder(self.transformer_layers, num_layers=4)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "vit_model = VisionTransformer(input_channels=3, num_classes=5)\n",
        "print(vit_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkZe_w0Z1kkz"
      },
      "source": [
        "#Combining Vit & ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKKezvOV3ngE"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from keras.layers import Dense, BatchNormalization, Dropout\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.callbacks import EarlyStopping\n",
        "\n",
        "# from keras.utils import to_categorical\n",
        "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# model_resNet50 = ResNet50_Transformer()\n",
        "# model_ViT = VisionTransformer()\n",
        "\n",
        "# \"\"\"LOADING WEIGHTS FROM PREVIOUSLY TRAINED MODELS (see files K_REPORT_MAIN_R50 resp. K_REPORT_MAIN_VGG)\"\"\"\n",
        "# model_resNet50.load_weights(\"K_R50_T2.h5\")\n",
        "# model_ViT.load_weights(\"K_VGG_T2.h5\")\n",
        "\n",
        "\n",
        "# \"\"\"MODEL COMBINATION\"\"\"\n",
        "# # creating the model extracting features from the last layer before the softmax layer.\n",
        "# vgg_extractor = Model(inputs=model_ViT.input, outputs=model_ViT.get_layer(\"dense_1024\").output)\n",
        "# r50_extractor = Model(inputs=model_resNet50.input, outputs=model_resNet50.get_layer(\"dense_1024\").output)\n",
        "\n",
        "# # using the model extractor to generate feature arrays with 1024 features from each model extractor.\n",
        "# y_train = to_categorical(training_set_V2.classes)\n",
        "# X_train_m1 = vgg_extractor.predict_generator(training_set_V2, steps = training_set_V2.n)\n",
        "# X_train_m2 = r50_extractor.predict_generator(training_set_V2, steps = training_set_V2.n)\n",
        "# X_train = np.concatenate([X_train_m1, X_train_m2], axis=1)\n",
        "\n",
        "# y_val = to_categorical(validation_set_V2.classes)\n",
        "# X_val_m1 = vgg_extractor.predict_generator(validation_set_V2, steps = validation_set_V2.n)\n",
        "# X_val_m2 = r50_extractor.predict_generator(validation_set_V2, steps = validation_set_V2.n)\n",
        "# X_val = np.concatenate([X_val_m1, X_val_m2], axis=1)\n",
        "\n",
        "# y_test = to_categorical(test_set_V2.classes)\n",
        "# X_test_m1 = vgg_extractor.predict_generator(test_set_V2, steps = test_set_V2.n)\n",
        "# X_test_m2 = r50_extractor.predict_generator(test_set_V2, steps = test_set_V2.n)\n",
        "# X_test = np.concatenate([X_test_m1, X_test_m2], axis=1)\n",
        "\n",
        "\n",
        "# \"\"\"TRAIN & TEST FEATURE EXTRACTION MODEL\"\"\"\n",
        "# # the feature arrays are read into a sequential model directly connecting them to the softmax layer, with\n",
        "# # some dropout and batch normalization in between.\n",
        "\n",
        "# # set a seed found to produce a run with consistently high validation accuracy (~.93 +)\n",
        "# # the specified seed will probably have to be adapted with different train-validation split\n",
        "# # and loaded model weights of R50 and VGG\n",
        "# np.random.seed(668)\n",
        "\n",
        "# opt = Adam(learning_rate=1e-3)\n",
        "# model = Sequential()\n",
        "# model.add(Dropout(0.75, input_shape=(2048,)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(5, activation=\"softmax\"))\n",
        "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "\n",
        "# epochs = 200\n",
        "# model.fit(X_train, y_train, batch_size=32, epochs=epochs, verbose=2, shuffle=True, validation_data=(X_val, y_val))\n",
        "\n",
        "# \"\"\"EVALUATE MODEL ON TEST DATA\"\"\"\n",
        "# y_test = np.argmax(y_test, axis=1)\n",
        "# pred = np.argmax(model.predict(X_test), axis=1)\n",
        "# print(accuracy_score(y_test, pred))\n",
        "# print(confusion_matrix(y_test, pred))\n",
        "# print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU9QC0uo3s6L"
      },
      "source": [
        "#Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4rpYxT7jWr5"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "bf6dJ7gDQz-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OxxX9eg1_2pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Dq-6rq0Z1P"
      },
      "outputs": [],
      "source": [
        "# # Initialize your Vision Transformer model\n",
        "# model = ResNet50_Transformer().to(device)\n",
        "\n",
        "# # Define loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adjust learning rate as needed\n",
        "\n",
        "# # Train the model (similar to previous example)\n",
        "# num_epochs = 10\n",
        "# for epoch in range(num_epochs):\n",
        "#     # Training loop\n",
        "#     model.train()\n",
        "#     for images, labels in train_loader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     # Calculate training accuracy\n",
        "#     train_accuracy = correct / total\n",
        "\n",
        "#     # Validation loop\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         val_loss = 0.0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "#         for images, labels in val_loader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             val_loss += loss.item()\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy}, Training Loss: {loss.item()}, Validation Accuracy: {correct/total}\")\n",
        "#     # Save model's state dictionary\n",
        "#     torch.save(model.state_dict(), '/content/drive/My Drive/resnet50_model_epoch_{}.h5'.format(epoch+1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Initialize your Vision Transformer model\n",
        "model = ResNet50_Transformer().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adjust learning rate as needed\n",
        "\n",
        "# Train the model (similar to previous example)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Collect predictions and true labels for confusion matrix\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy}, Training Loss: {loss.item()}, Validation Accuracy: {correct/total}\")\n",
        "    # Save model's state dictionary\n",
        "    torch.save(model.state_dict(), '/content/drive/My Drive/resnet50_model_epoch_{}.h5'.format(epoch+1))\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHsn9P8Cg__s",
        "outputId": "256c305f-a787-4bc7-d674-2f887567a442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Accuracy: 0.912828947368421, Training Loss: 0.1986999809741974, Validation Accuracy: 0.912828947368421\n",
            "Confusion Matrix:\n",
            "[[119   3   0   0   0]\n",
            " [ 10 114   2   0   1]\n",
            " [  2  13  92   6   0]\n",
            " [  4   0   0 107   1]\n",
            " [  0   8   3   0 123]]\n",
            "Epoch 2/10, Training Accuracy: 0.9013157894736842, Training Loss: 0.27588513493537903, Validation Accuracy: 0.9013157894736842\n",
            "Confusion Matrix:\n",
            "[[102  19   0   0   1]\n",
            " [  1 122   3   0   1]\n",
            " [  0  24  86   1   2]\n",
            " [  0   2   1 107   2]\n",
            " [  0   3   0   0 131]]\n",
            "Epoch 3/10, Training Accuracy: 0.9407894736842105, Training Loss: 0.022009538486599922, Validation Accuracy: 0.9407894736842105\n",
            "Confusion Matrix:\n",
            "[[119   3   0   0   0]\n",
            " [ 11 101  10   0   5]\n",
            " [  1   2 107   0   3]\n",
            " [  0   0   0 112   0]\n",
            " [  0   1   0   0 133]]\n",
            "Epoch 4/10, Training Accuracy: 0.9325657894736842, Training Loss: 0.015569234266877174, Validation Accuracy: 0.9325657894736842\n",
            "Confusion Matrix:\n",
            "[[111   8   0   3   0]\n",
            " [  3 117   5   1   1]\n",
            " [  0   8  99   6   0]\n",
            " [  0   0   0 112   0]\n",
            " [  0   5   0   1 128]]\n",
            "Epoch 5/10, Training Accuracy: 0.9424342105263158, Training Loss: 0.42616522312164307, Validation Accuracy: 0.9424342105263158\n",
            "Confusion Matrix:\n",
            "[[120   1   0   0   1]\n",
            " [ 11 110   3   0   3]\n",
            " [  1   9 103   0   0]\n",
            " [  0   0   1 111   0]\n",
            " [  0   3   2   0 129]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0hGYKps1pko"
      },
      "source": [
        "#Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_auOZS6e1ms0"
      },
      "outputs": [],
      "source": [
        "# Test the model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vit Model"
      ],
      "metadata": {
        "id": "gMGRjdoLBIgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your Vision Transformer model\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model = VisionTransformer(input_channels=3, num_classes=10).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit_model.parameters(), lr=0.0001)  # Adjust learning rate as needed\n",
        "\n",
        "# Dummy data loaders for illustration purposes\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "-ePxH6TKyf__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the correct counter\n",
        "correct = 0\n",
        "\n",
        "# Train the model (similar to previous example)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Initialize total for calculating training accuracy\n",
        "    total = 0\n",
        "\n",
        "    # Training loop\n",
        "    vit_model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    vit_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = vit_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy}, Training Loss: {loss.item()}, Validation Accuracy: {correct/total}\")\n",
        "    # Save model's state dictionary\n",
        "    torch.save(vit_model.state_dict(), '/content/drive/My Drive/viT_model_epoch_{}.h5'.format(epoch+1))"
      ],
      "metadata": {
        "id": "iY3hIHOTx9tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on the test set\n",
        "vit_model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation as we're only evaluating the model\n",
        "    test_loss = 0.0  # Initialize the test loss\n",
        "    correct = 0  # Initialize the number of correct predictions\n",
        "    total = 0  # Initialize the total number of samples\n",
        "\n",
        "    # Loop through the test set\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to the device (e.g., GPU)\n",
        "        outputs = vit_model(images)  # Forward pass\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()  # Accumulate the test loss\n",
        "\n",
        "        # Calculate the number of correct predictions\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the predicted labels\n",
        "        total += labels.size(0)  # Increment the total count by the batch size\n",
        "        correct += (predicted == labels).sum().item()  # Increment the correct count by the number of correct predictions\n",
        "\n",
        "    # Calculate and print the average test loss and test accuracy\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {correct/total}\")\n"
      ],
      "metadata": {
        "id": "ZjemhpC7QMrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "model = ResNet50_Transformer().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy}, Training Loss: {loss.item()}, Validation Accuracy: {correct/total}\")\n",
        "    torch.save(model.state_dict(), '/content/drive/My Drive/resnet50_model_epoch_{}.h5'.format(epoch+1))\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {correct/total}\")\n",
        "\n",
        "vit_model = VisionTransformer(input_channels=3, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit_model.parameters(), lr=0.0001)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "correct = 0\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    vit_model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "    vit_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = vit_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy}, Training Loss: {loss.item()}, Validation Accuracy: {correct/total}\")\n",
        "    torch.save(vit_model.state_dict(), '/content/drive/My Drive/viT_model_epoch_{}.h5'.format(epoch+1))\n",
        "\n",
        "vit_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = vit_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {correct/total}\")\n"
      ],
      "metadata": {
        "id": "dEQ2-MYdjpix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot Performance"
      ],
      "metadata": {
        "id": "qGoQJ81_DJkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def plot_performance(train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "#     plt.figure(figsize=(12, 5))\n",
        "\n",
        "#     # Plot losses\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(train_losses, label='Training Loss')\n",
        "#     plt.plot(val_losses, label='Validation Loss')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.title('Training and Validation Loss')\n",
        "#     plt.legend()\n",
        "\n",
        "#     # Plot accuracies\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(train_accuracies, label='Training Accuracy')\n",
        "#     plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Accuracy')\n",
        "#     plt.title('Training and Validation Accuracy')\n",
        "#     plt.legend()\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "Pajp_WoSDIss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot performance\n",
        "# plot_performance(train_losses, val_losses, train_accuracies, val_accuracies)"
      ],
      "metadata": {
        "id": "wNMUAnwkDSNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targ_vals_int = list()\n",
        "for i in targ_vals:\n",
        "  targ_vals_int.append(int(i))\n",
        "\n",
        "pred_vals_int = list()\n",
        "for i in pred_vals:\n",
        "  pred_vals_int.append(int(i))"
      ],
      "metadata": {
        "id": "lupHxa3Ffr4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_vals_nonround_int = list()\n",
        "for i in pred_vals_nonround:\n",
        "  pred_vals_nonround_int.append(float(i))"
      ],
      "metadata": {
        "id": "EkTm9YFtfxGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix as matrix_print_graph_confusion\n",
        "\n",
        "import matplotlib.pyplot as plotter\n",
        "matrix_confuse = matrix_print_graph_confusion(targ_vals_int, pred_vals_int)\n",
        "print(matrix_confuse)\n",
        "\n",
        "import seaborn\n",
        "\n",
        "seaborn.heatmap(matrix_confuse, annot=True)\n",
        "plotter.xlabel('Values predicted')\n",
        "plotter.ylabel('Values Expected')\n",
        "plotter.show()"
      ],
      "metadata": {
        "id": "mlhY66ysfe4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr = [0.1986999809741974,0.27588513493537903, 0.022009538486599922, 0.015569234266877174, 0.4261652231216430]\n",
        "accuracy = []"
      ],
      "metadata": {
        "id": "XkgARyDdfi2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}